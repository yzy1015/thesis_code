{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import scipy.optimize\n",
    "import roboschool\n",
    "from agent_file_random_mask import agent\n",
    "import numpy as np\n",
    "import random\n",
    "from expert import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from gym import spaces\n",
    "plt.style.use('seaborn-white')\n",
    "sns.set(context = \"paper\", font = \"monospace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class adversial_env(object):\n",
    "    def __init__(self,runst=True):\n",
    "        # parameter\n",
    "        self.env = gym.make(\"RoboschoolInvertedPendulum-v1\")\n",
    "        self.env.seed(0)\n",
    "        self.max_turn = 1000\n",
    "        self.combine_ratio = 0.05\n",
    "        self.mask = np.random.choice([1],size = [1,10])\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.env.observation_space.shape[0],))\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(2*self.env.observation_space.shape[0],))\n",
    "        self.agent = SmallReactivePolicy(self.env.observation_space, self.env.action_space) # declare sample trained agent\n",
    "        self.obsr = 0\n",
    "        self.epi_num = 0\n",
    "        self.total_score = 0\n",
    "        self.first = True\n",
    "        self.run_avg = 0\n",
    "        self.rvg_list = []\n",
    "        self.score_list = []\n",
    "        self.epi_list = []\n",
    "        self.run_st = runst\n",
    "    \n",
    "    # define reward function\n",
    "    def reward(self, st):\n",
    "        return np.abs(st[3])-0.08#(np.abs(st[3])-0.00786861)*100\n",
    "    \n",
    "    def step(self, a):\n",
    "        self.epi_num = self.epi_num + 1\n",
    "        \n",
    "        ac = self.agent.act(a)\n",
    "        self.obsr, r, done, _ = self.env.step(ac)\n",
    "        self.obsr = np.concatenate((self.obsr, self.obsr), axis=0)\n",
    "        if self.epi_num >= self.max_turn:\n",
    "            done = True\n",
    "        \n",
    "        if self.first and done:\n",
    "            self.first = False\n",
    "            self.run_avg = self.total_score\n",
    "            self.score_list = [self.total_score]\n",
    "            self.epi_list = [self.epi_num]\n",
    "            print(self.run_avg, self.score_list, self.epi_list)\n",
    "        \n",
    "        final_r = r \n",
    "        \n",
    "        self.total_score += final_r\n",
    "        \n",
    "        if self.epi_num%300 == 299:\n",
    "            self.mask = np.random.choice([1],size = [1,10])\n",
    "            self.mask[0,np.random.randint(0,9, size=1)[0]] = 0 \n",
    "        \n",
    "        if self.run_st:\n",
    "            self.obser_noise = self.obsr\n",
    "        else:        \n",
    "            self.obser_noise = (self.obsr*self.mask)[0]\n",
    "            \n",
    "        return self.obser_noise, final_r, done, 0\n",
    "    \n",
    "    def non_adstep(self, a):\n",
    "        self.epi_num = self.epi_num + 1\n",
    "        ac = self.agent.act(a)\n",
    "        self.obsr, r, done, _ = self.env.step(ac)\n",
    "        self.obsr = np.concatenate((self.obsr, self.obsr), axis=0)\n",
    "        \n",
    "        if self.epi_num >= self.max_turn:\n",
    "            done = True\n",
    "        \n",
    "        if self.first and done:\n",
    "            self.first = False\n",
    "            self.run_avg = self.total_score\n",
    "        \n",
    "        final_r = r \n",
    "        \n",
    "        self.total_score += final_r\n",
    "        \n",
    "        obser = (self.obsr)\n",
    "        \n",
    "        return obser, final_r, done, 0\n",
    "        \n",
    "        \n",
    "    def seed(self, a):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.mask = np.random.choice([1],size = [1,10])\n",
    "        self.obsr = self.env.reset()\n",
    "        self.obsr = np.concatenate((self.obsr, self.obsr), axis=0)\n",
    "        #print(self.total_score)\n",
    "        self.run_avg = (self.combine_ratio*self.total_score) + (1-self.combine_ratio)*self.run_avg\n",
    "        #print(self.run_avg)\n",
    "        #print(self.epi_num)\n",
    "        \n",
    "        if not self.first: #########################################\n",
    "            self.rvg_list.append(self.run_avg)\n",
    "            self.score_list.append(self.total_score)\n",
    "            self.epi_list.append(self.epi_num)\n",
    "        \n",
    "        self.epi_num = 0\n",
    "        self.total_score = 0\n",
    "        return self.obsr\n",
    "    \n",
    "    def result_plot(self):\n",
    "        fon_size = 19\n",
    "        fig=plt.figure(figsize=(18, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(self.score_list[1:])\n",
    "        plt.xlabel('episodes',fontsize=fon_size)\n",
    "        plt.ylabel('total reward',fontsize=fon_size)\n",
    "        #plt.subplot(1,3,2)\n",
    "        #plt.plot(self.rvg_list[1:])\n",
    "        #plt.xlabel('episodes')\n",
    "        #plt.ylabel('running average reward')\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(self.epi_list)\n",
    "        plt.xlabel('episodes',fontsize=fon_size)\n",
    "        plt.ylabel('time steps',fontsize=fon_size)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define an observer (new agent) and save running state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class par(object):\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.995\n",
    "        self.env_name = \"Reacher-v1\"\n",
    "        self.tau = 0.97\n",
    "        self.l2_reg = 1e-3\n",
    "        self.max_kl = 1e-2\n",
    "        self.damping = 1e-1\n",
    "        self.seed = 543\n",
    "        self.batch_size = 55000\n",
    "        self.max_epi = 6000\n",
    "        self.log_interval = 1\n",
    "        self.max_avg = 999\n",
    "        \n",
    "args = par()\n",
    "env = adversial_env()\n",
    "agn = agent(env, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agn.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agn.save_model('agent_model/random_period_mask_observer_double_sensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class par(object):\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.995\n",
    "        self.env_name = \"Reacher-v1\"\n",
    "        self.tau = 0.97\n",
    "        self.l2_reg = 1e-3\n",
    "        self.max_kl = 1e-2\n",
    "        self.damping = 1e-1\n",
    "        self.seed = 543\n",
    "        self.batch_size = 55000\n",
    "        self.max_epi = 6000\n",
    "        self.log_interval = 1\n",
    "        self.max_avg = 999\n",
    "        \n",
    "args = par()\n",
    "env = adversial_env(runst=False)\n",
    "agn = agent(env, args)\n",
    "agn.load_model('agent_model/random_period_mask_observer_double_sensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agn.learn(run_state_update = False) # disable running state update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.result_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = adversial_env()\n",
    "for i in range(1000):\n",
    "    score = 0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    itr = 0\n",
    "    do = False\n",
    "    while done == False:   \n",
    "        a = agn.select_action_deterministic(obs)\n",
    "        obs, r, done, _ = env.step(a)\n",
    "        if done:\n",
    "            do = True\n",
    "\n",
    "        score += r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.result_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance on non_adversarial environment with observer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = adversial_env()\n",
    "for i in range(1000):\n",
    "    score = 0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    itr = 0\n",
    "    do = False\n",
    "    while done == False:   \n",
    "        a = agn.select_action_deterministic(obs)\n",
    "        \n",
    "        obs, r, done, _ = env.non_adstep(a)\n",
    "        if done:\n",
    "            do = True\n",
    "\n",
    "        score += r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.result_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agn.save_model('agent_model/random_period_mask_observer_double_sensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
