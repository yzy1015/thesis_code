{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /tmp/openai-2018-05-04-19-21-08-308141\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './py_torch_trpo')\n",
    "from baselines.common import set_global_seeds, tf_util as U\n",
    "import gym\n",
    "import roboschool\n",
    "import numpy as np\n",
    "import random\n",
    "from expert import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from gym import spaces\n",
    "from base_line_model.TRPO_agent import TRPO_agent_new\n",
    "from base_line_model.mlp import MlpPolicy_new\n",
    "from baselines import logger\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "sns.set(context = \"paper\", font = \"monospace\", font_scale=2)\n",
    "\n",
    "seed = 1\n",
    "logger.configure()\n",
    "U.make_session(num_cpu=16).__enter__()\n",
    "set_global_seeds(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class action_space(object):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.high = np.array([ 1,  1,  1,  1,  1])\n",
    "        self.low = -np.array([ 1,  1,  1,  1,  1])\n",
    "        self.shape = env.observation_space.shape\n",
    "    \n",
    "    def sample(self):\n",
    "    \n",
    "        return self.env.observation_space.sample()\n",
    "        \n",
    "        \n",
    "class adversial_env(object):\n",
    "    def __init__(self):\n",
    "        # parameter\n",
    "        self.env = gym.make(\"RoboschoolInvertedPendulum-v1\")\n",
    "        self.env.seed(0)\n",
    "        self.ratio = 0.7\n",
    "        self.threshold = np.array([ 0.14244403,  0.07706523,  0.00016789,  0.00789366,  0.02395424])\n",
    "        self.max_turn = 1000\n",
    "        self.combine_ratio = 0.05\n",
    "        \n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.env.observation_space.shape[0],))\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.agent = SmallReactivePolicy(self.env.observation_space, self.env.action_space) # declare sample trained agent\n",
    "        self.obsr = 0\n",
    "        self.epi_num = 0\n",
    "        self.total_score = 0\n",
    "        self.first = True\n",
    "        self.run_avg = 0\n",
    "        self.rvg_list = []\n",
    "        self.score_list = []\n",
    "        self.epi_list = []\n",
    "        self.env.metadata\n",
    "    \n",
    "    # define reward function\n",
    "    def reward(self, st):\n",
    "        return np.abs(st[3])-0.08 #np.abs(st[3])+0.2*np.abs(st[1])-0.08#\n",
    "    \n",
    "    def step(self, a):\n",
    "        self.epi_num = self.epi_num + 1\n",
    "        \n",
    "        obs = np.clip(a,-1,1)*self.threshold*self.ratio + self.obsr\n",
    "        ac = self.agent.act(obs)\n",
    "        self.obsr, r, done, _ = self.env.step(ac)\n",
    "        #print( np.clip(a,-1,1),np.clip(a,-1,1)*self.ratio)\n",
    "        \n",
    "        if self.epi_num >= self.max_turn:\n",
    "            done = True\n",
    "        \n",
    "        if self.first and done: ###################################\n",
    "            self.first = False\n",
    "            self.run_avg = self.total_score\n",
    "            self.score_list = [self.total_score]\n",
    "            self.epi_list = [self.epi_num]\n",
    "            print(self.run_avg, self.score_list, self.epi_list)\n",
    "            \n",
    "        \n",
    "        final_r = self.reward(self.obsr)\n",
    "        if done and self.epi_num < self.max_turn:\n",
    "            final_r = 15 # terminal cost \n",
    "        \n",
    "        self.total_score += final_r\n",
    "        return self.obsr, final_r, done, 0\n",
    "        \n",
    "        \n",
    "    def seed(self, a):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.obsr = self.env.reset()\n",
    "        #print(self.total_score)\n",
    "        self.run_avg = (self.combine_ratio*self.total_score) + (1-self.combine_ratio)*self.run_avg\n",
    "        #print(self.run_avg)\n",
    "        #print(self.epi_num)\n",
    "        \n",
    "        if not self.first: #########################################\n",
    "            self.rvg_list.append(self.run_avg)\n",
    "            self.score_list.append(self.total_score)\n",
    "            self.epi_list.append(self.epi_num)\n",
    "        \n",
    "        self.epi_num = 0\n",
    "        self.total_score = 0\n",
    "        return self.obsr\n",
    "    \n",
    "    def result_plot(self):\n",
    "        fon_size = 19\n",
    "        x = list(range(0, len(self.score_list[1:])))\n",
    "        fig=plt.figure(figsize=(18, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.scatter(x,self.score_list[1:], s=5)\n",
    "        plt.xlabel('episodes',fontsize=fon_size)\n",
    "        plt.ylabel('cumulative reward',fontsize=fon_size)\n",
    "        plt.ylim([-80,20])\n",
    "        #plt.subplot(1,3,2)\n",
    "        #plt.plot(self.rvg_list[1:])\n",
    "        #plt.xlabel('episodes')\n",
    "        #plt.ylabel('running average reward')\n",
    "        plt.subplot(1,2,2)\n",
    "        x = list(range(0, len(self.epi_list)))\n",
    "        plt.scatter(x,self.epi_list, s=5)\n",
    "        plt.xlabel('episodes',fontsize=fon_size)\n",
    "        plt.ylabel('time steps',fontsize=fon_size)\n",
    "        plt.ylim([0,1200])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define an adversary(new agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-04 19:21:08,493] Making new env: RoboschoolInvertedPendulum-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init param sum 1.91204\n",
      "INFO:tensorflow:Restoring parameters from adversary/data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-04 19:21:09,764] Restoring parameters from adversary/data\n"
     ]
    }
   ],
   "source": [
    "env2 = adversial_env()\n",
    "class pargm(object):\n",
    "    def __init__(self):\n",
    "        self.timesteps_per_batch = 50000 # what to train on\n",
    "        self.max_kl = 0.01\n",
    "        self.cg_iters = 10\n",
    "        self.gamma = 0.995\n",
    "        self.lam =  0.97# advantage estimation\n",
    "        self.entcoeff=0.0\n",
    "        self.cg_damping=0.1\n",
    "        self.vf_stepsize=1e-3\n",
    "        self.vf_iters =5\n",
    "        self.max_timesteps = 1e8\n",
    "        self.max_episodes=0\n",
    "        self.max_iters=0  # time constraint\n",
    "        self.max_epi_avg = 1001\n",
    "        self.callback=None\n",
    "\n",
    "\n",
    "def policy_fn(name, ob_space, ac_space):\n",
    "        return MlpPolicy_new(name=name, ob_space=ob_space, ac_space=ac_space,\n",
    "            hid_size=128, num_hid_layers=2)\n",
    "    \n",
    "parg = pargm()\n",
    "agn = TRPO_agent_new('pi1', env2, policy_fn, parg)\n",
    "agn.restore('adversary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agn.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2.result_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-05-04 19:21:29,443] Making new env: RoboschoolInvertedPendulum-v1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-27.598428835 [-27.598428835013632] [499]\n"
     ]
    }
   ],
   "source": [
    "env = adversial_env()\n",
    "for i in range(1000):\n",
    "    score = 0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    itr = 0\n",
    "    do = False\n",
    "    time = 0\n",
    "    while done == False:   \n",
    "        a = agn.action_ev(obs)\n",
    "        time +=1\n",
    "        obs, r, done, _ = env.step(a)\n",
    "        if done:\n",
    "            do = True\n",
    "\n",
    "        score += r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.result_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agn.save_data('adversary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adversarial evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import scipy.optimize\n",
    "import roboschool\n",
    "from agent_file import agent\n",
    "import numpy as np\n",
    "import random\n",
    "from expert import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from gym import spaces\n",
    "plt.style.use('seaborn-white')\n",
    "sns.set(context = \"paper\", font = \"monospace\", font_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class pargm(object):\n",
    "    def __init__(self):\n",
    "        self.timesteps_per_batch = 50000 # what to train on\n",
    "        self.max_kl = 0.01\n",
    "        self.cg_iters = 10\n",
    "        self.gamma = 0.995\n",
    "        self.lam =  0.97# advantage estimation\n",
    "        self.entcoeff=0.0\n",
    "        self.cg_damping=0.1\n",
    "        self.vf_stepsize=1e-3\n",
    "        self.vf_iters =5\n",
    "        self.max_timesteps = 1e8\n",
    "        self.max_episodes=0\n",
    "        self.max_iters=0  # time constraint\n",
    "        self.max_epi_avg = 1001\n",
    "        self.callback=None\n",
    "\n",
    "\n",
    "def policy_fn(name, ob_space, ac_space):\n",
    "        return MlpPolicy_new(name=name, ob_space=ob_space, ac_space=ac_space,\n",
    "            hid_size=128, num_hid_layers=2)\n",
    "    \n",
    "parg = pargm()\n",
    "agn = TRPO_agent_new('pi1', env2, policy_fn, parg)\n",
    "agn.restore('adversary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = adversial_env()\n",
    "action = []\n",
    "for i in range(1000):\n",
    "    score = 0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    itr = 0\n",
    "    do = False\n",
    "    time = 0\n",
    "    while done == False:   \n",
    "        a = agn.action_ev(obs)\n",
    "        action.append(np.clip(a,-1,1)*np.array([ 0.14244403,  0.07706523,  0.00016789,  0.00789366,  0.02395424]))*0.7\n",
    "        time +=1\n",
    "        obs, r, done, _ = env.step(a)\n",
    "        if done:\n",
    "            do = True\n",
    "\n",
    "        score += r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.result_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_num = 50\n",
    "sensor_array = np.array(action)\n",
    "fon_size = 17\n",
    "sns.set(context = \"paper\", font = \"monospace\", font_scale=1.3)\n",
    "fig=plt.figure(figsize=(18, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(1,5,1)\n",
    "plt.hist(sensor_array[:,0], bins = bin_num)\n",
    "plt.xlabel('x', fontsize=fon_size)\n",
    "plt.ylabel('frequency', fontsize=fon_size)\n",
    "\n",
    "plt.subplot(1,5,2)\n",
    "plt.hist(sensor_array[:,1], bins = bin_num)\n",
    "plt.xlabel('vx', fontsize=fon_size)\n",
    "\n",
    "plt.subplot(1,5,3)\n",
    "plt.hist(sensor_array[:,2], bins = bin_num)\n",
    "plt.xlabel(r'cos $\\theta$', fontsize=fon_size)\n",
    "\n",
    "plt.subplot(1,5,4)\n",
    "plt.hist(sensor_array[:,3], bins = bin_num)\n",
    "plt.xlabel(r'sin $\\theta$', fontsize=fon_size)\n",
    "\n",
    "plt.subplot(1,5,5)\n",
    "plt.hist(sensor_array[:,4], bins = bin_num)\n",
    "plt.xlabel(r'$\\dot \\theta$', fontsize=fon_size)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
