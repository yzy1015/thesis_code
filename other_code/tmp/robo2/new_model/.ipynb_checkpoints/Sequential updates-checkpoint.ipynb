{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './py_torch_trpo')\n",
    "from baselines.common import set_global_seeds, tf_util as U\n",
    "import gym\n",
    "import roboschool\n",
    "import numpy as np\n",
    "import random\n",
    "from expert import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from gym import spaces\n",
    "from base_line_model.TRPO_agent import TRPO_agent_new\n",
    "from base_line_model.mlp import MlpPolicy_new\n",
    "from baselines import logger\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "sns.set(context = \"paper\", font = \"monospace\", font_scale=2)\n",
    "seed = 1\n",
    "logger.configure()\n",
    "U.make_session(num_cpu=16).__enter__()\n",
    "set_global_seeds(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy observer and adversary agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dummy class, use for construction of adversary and observer environment\n",
    "class dummy_adversary_env(object):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"RoboschoolInvertedPendulum-v1\")\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.env.observation_space.shape[0],))\n",
    "        self.observation_space = self.env.observation_space\n",
    "        \n",
    "    def action_ev(self, s):\n",
    "        return action_space.sample()\n",
    "    \n",
    "class dummy_observer_env(object):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"RoboschoolInvertedPendulum-v1\")\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.env.observation_space.shape[0],))\n",
    "        self.observation_space = self.env.observation_space\n",
    "    \n",
    "    def action_ev(self, s):\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversary environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class adversary_env(object):\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"RoboschoolInvertedPendulum-v1\")\n",
    "        self.threshold = np.array([ 0.14244403,  0.07706523,  0.00016789,  0.00789366,  0.02395424]) # standard deviation\n",
    "        # pre-trained controller\n",
    "        self.agent = SmallReactivePolicy(self.env.observation_space, self.env.action_space) # declare sample trained agent\n",
    "        self.env.seed(0)\n",
    "        \n",
    "        # observer agent\n",
    "        self.observer = dummy_observer_env()\n",
    "        \n",
    "        self.ratio = 0.7\n",
    "        self.max_turn = 1000\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.env.observation_space.shape[0],))\n",
    "        self.observation_space = self.env.observation_space\n",
    "        \n",
    "        self.obsr = 0\n",
    "        self.epi_num = 0\n",
    "        self.total_score = 0\n",
    "        self.first = True\n",
    "        self.score_list = []\n",
    "        self.epi_list = []\n",
    "        self.env.metadata\n",
    "    \n",
    "    # -----------------------   define reward for adversary agent --------------------------------- \n",
    "    def reward(self, st):\n",
    "        return np.abs(st[3]) - 0.08 # sin theta - 0.8\n",
    "    \n",
    "    def step(self, a):\n",
    "        self.epi_num = self.epi_num + 1\n",
    "        obs = np.clip(a,-1,1)*self.threshold*self.ratio + self.obsr\n",
    "        \n",
    "        # observer take the state input\n",
    "        obs = self.observer.action_ev(obs)\n",
    "        \n",
    "        ac = self.agent.act(obs)\n",
    "        self.obsr, r, done, _ = self.env.step(ac)\n",
    "        \n",
    "        if self.epi_num >= self.max_turn:\n",
    "            done = True\n",
    "        \n",
    "        if self.first and done:\n",
    "            self.first = False\n",
    "            self.score_list = [self.total_score]\n",
    "            self.epi_list = [self.epi_num]\n",
    "            \n",
    "        final_r = self.reward(self.obsr)\n",
    "        if done and self.epi_num < self.max_turn:\n",
    "            final_r = 15 # terminal cost \n",
    "        \n",
    "        self.total_score += final_r\n",
    "        return self.obsr, final_r, done, 0\n",
    "        \n",
    "        \n",
    "    def seed(self, a):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.obsr = self.env.reset()        \n",
    "        if not self.first: \n",
    "            self.score_list.append(self.total_score)\n",
    "            self.epi_list.append(self.epi_num)\n",
    "        \n",
    "        self.epi_num = 0\n",
    "        self.total_score = 0\n",
    "        return self.obsr\n",
    "    \n",
    "    def env_reset(self):\n",
    "        self.obsr = 0\n",
    "        self.epi_num = 0\n",
    "        self.total_score = 0\n",
    "        self.first = True\n",
    "        self.score_list = []\n",
    "        self.epi_list = []\n",
    "    \n",
    "    def result_plot(self):\n",
    "        fon_size = 19\n",
    "        x = list(range(0, len(self.score_list[1:])))\n",
    "        fig=plt.figure(figsize=(18, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.scatter(x,self.score_list[1:], s=5)\n",
    "        plt.xlabel('episodes',fontsize=fon_size)\n",
    "        plt.ylabel('cumulative reward',fontsize=fon_size)\n",
    "        plt.subplot(1,2,2)\n",
    "        x = list(range(0, len(self.epi_list)))\n",
    "        plt.scatter(x,self.epi_list, s=5)\n",
    "        plt.xlabel('episodes',fontsize=fon_size)\n",
    "        plt.ylabel('time steps',fontsize=fon_size)\n",
    "        plt.ylim([0,1200])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observer environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class observer_env(object):\n",
    "    agn_index = 1\n",
    "    def __init__(self):\n",
    "        # parameter\n",
    "        self.env = gym.make(\"RoboschoolInvertedPendulum-v1\")\n",
    "        self.env.seed(0)\n",
    "        # ------------------ pre-trained agent -----------------------------\n",
    "        self.agent = SmallReactivePolicy(self.env.observation_space, self.env.action_space) # declare sample trained agent\n",
    "        self.ratio = 0.7\n",
    "        self.threshold = np.array([ 0.14244403,  0.07706523,  0.00016789,  0.00789366,  0.02395424])\n",
    "        \n",
    "        # ------ dummy adversarial agent -------------------\n",
    "        self.adv_agn = dummy_adversary_env()\n",
    "        \n",
    "        self.max_turn = 1000\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.env.observation_space.shape[0],))\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.obsr = 0\n",
    "        self.epi_num = 0\n",
    "        self.total_score = 0\n",
    "        self.first = True\n",
    "        self.score_list = []\n",
    "        self.epi_list = []\n",
    "        self.env.metadata\n",
    "        \n",
    "    # define reward function\n",
    "    def reward(self, st):\n",
    "        return 1 #np.abs(st[3])-0.08#(np.abs(st[3])-0.00786861)*100\n",
    "    \n",
    "    def step(self, a):\n",
    "        self.epi_num = self.epi_num + 1\n",
    "        \n",
    "        ac = self.agent.act(a)\n",
    "        self.obsr, r, done, _ = self.env.step(ac)\n",
    "        \n",
    "        if self.epi_num >= self.max_turn:\n",
    "            done = True\n",
    "        \n",
    "        if self.first and done: ###################################\n",
    "            self.first = False\n",
    "            self.score_list = [self.total_score]\n",
    "            self.epi_list = [self.epi_num]\n",
    "        \n",
    "        final_r = self.reward(self.obsr)\n",
    "        \n",
    "        self.total_score += final_r\n",
    "        \n",
    "        # return noise output (adversarial)\n",
    "        action = self.adv_agn.action_ev(self.obsr)\n",
    "        obs = np.clip(action,-1,1)*self.threshold*self.ratio + self.obsr\n",
    "        return obs, final_r, done, 0\n",
    "    \n",
    "    \n",
    "    def seed(self, a):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.obsr = self.env.reset()\n",
    "        if not self.first:\n",
    "            self.score_list.append(self.total_score)\n",
    "            self.epi_list.append(self.epi_num)\n",
    "        \n",
    "        self.epi_num = 0\n",
    "        self.total_score = 0\n",
    "        return self.obsr\n",
    "    \n",
    "    def env_reset(self):\n",
    "        self.obsr = 0\n",
    "        self.epi_num = 0\n",
    "        self.total_score = 0\n",
    "        self.first = True\n",
    "        self.score_list = []\n",
    "        self.epi_list = []\n",
    "    \n",
    "    def result_plot(self):\n",
    "        fon_size = 19\n",
    "        x = list(range(0, len(self.score_list)))\n",
    "        fig=plt.figure(figsize=(18, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.scatter(x,self.score_list, s=5)\n",
    "        plt.xlabel('episodes',fontsize=fon_size)\n",
    "        plt.ylabel('cumulative reward',fontsize=fon_size)\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        x = list(range(0, len(self.epi_list)))\n",
    "        plt.scatter(x,self.epi_list, s=5)\n",
    "        plt.xlabel('episodes',fontsize=fon_size)\n",
    "        plt.ylabel('time steps',fontsize=fon_size)\n",
    "        plt.ylim([0,1200])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversary agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_env1 = dummy_adversary_env()\n",
    "class pargm1(object):\n",
    "    def __init__(self):\n",
    "        self.timesteps_per_batch = 5000 # what to train on\n",
    "        self.max_kl = 0.01\n",
    "        self.cg_iters = 10\n",
    "        self.gamma = 0.995\n",
    "        self.lam =  0.97# advantage estimation\n",
    "        self.entcoeff=0.0\n",
    "        self.cg_damping=0.1\n",
    "        self.vf_stepsize=1e-3\n",
    "        self.vf_iters =5\n",
    "        self.max_timesteps = 5000\n",
    "        self.max_episodes=0\n",
    "        self.max_iters=0  # time constraint\n",
    "        self.max_epi_avg = 1000\n",
    "        self.callback=None\n",
    "\n",
    "def policy_fn1(name, ob_space, ac_space):\n",
    "        return MlpPolicy_new(name=name, ob_space=ob_space, ac_space=ac_space,\n",
    "            hid_size=128, num_hid_layers=2)\n",
    "    \n",
    "parg = pargm1()\n",
    "adversary = TRPO_agent_new('pi1', dummy_env1, policy_fn1, parg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adversary.restore(\"adversary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observer agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_env2 = dummy_observer_env()\n",
    "class pargm2(object):\n",
    "    def __init__(self):\n",
    "        self.timesteps_per_batch = 5000 # what to train on\n",
    "        self.max_kl = 0.01\n",
    "        self.cg_iters = 10\n",
    "        self.gamma = 0.995\n",
    "        self.lam =  0.97# advantage estimation\n",
    "        self.entcoeff=0.0\n",
    "        self.cg_damping=0.1\n",
    "        self.vf_stepsize=1e-3\n",
    "        self.vf_iters =5\n",
    "        self.max_timesteps = 5000\n",
    "        self.max_episodes=0\n",
    "        self.max_iters=0  # time constraint\n",
    "        self.max_epi_avg = 1001\n",
    "        self.callback=None\n",
    "\n",
    "def policy_fn2(name, ob_space, ac_space):\n",
    "        return MlpPolicy_new(name=name, ob_space=ob_space, ac_space=ac_space,\n",
    "            hid_size=128, num_hid_layers=2)\n",
    "\n",
    "    \n",
    "parg = pargm2()\n",
    "observer = TRPO_agent_new('pi2', dummy_env2, policy_fn2, parg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observer.restore(\"observer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_env = observer_env()\n",
    "adv_env = adversary_env()\n",
    "# replace all dummy instance\n",
    "obs_env.adv_agn = adversary\n",
    "adv_env.observer = observer\n",
    "adversary.env = adv_env\n",
    "observer.env = obs_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    print(\"\\n -------------- adversary update -------------- \\n\")\n",
    "    adversary.learn()\n",
    "    print(\"\\n -------------- observer update -------------- \\n\")\n",
    "    observer.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
