{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './py_torch_trpo')\n",
    "from agent_file import agent\n",
    "import gym\n",
    "import scipy.optimize\n",
    "import roboschool\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from gym import spaces\n",
    "plt.style.use('seaborn-white')\n",
    "sns.set(context = \"paper\", font = \"monospace\", font_scale=2)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-white')\n",
    "from baselines.common import set_global_seeds, tf_util as U\n",
    "from baselines import bench\n",
    "import gym, logging\n",
    "import roboschool\n",
    "from baselines import logger\n",
    "from baselines.ppo1 import mlp_policy, pposgd_simple\n",
    "from base_line_model.mlp import MlpPolicy_new\n",
    "from base_line_model.PPO_agent import learning_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"RoboschoolWalker2d-v1\")\n",
    "env.seed(0)\n",
    "np.random.seed(0)\n",
    "seed = 1\n",
    "logger.configure()\n",
    "U.make_session(num_cpu=16).__enter__()\n",
    "set_global_seeds(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = gym.make('RoboschoolWalker2d-v1')\n",
    "env2 = bench.Monitor(env2, logger.get_dir(),allow_early_resets=True)\n",
    "env2.seed(seed)\n",
    "gym.logger.setLevel(logging.WARN)\n",
    "class pargm(object):\n",
    "    def __init__(self):\n",
    "        self.timesteps_per_actorbatch = 25000#25000 # timesteps per actor per update\n",
    "        self.clip_param = 0.2 \n",
    "        self.entcoeff = 0.0 # clipping parameter epsilon, entropy coeff\n",
    "        self.optim_epochs = 10 \n",
    "        self.optim_stepsize = 3e-4\n",
    "        self.optim_batchsize = 64# optimization hypers\n",
    "        self.gamma = 0.99\n",
    "        self.lam = 0.95 # advantage estimation\n",
    "        self.max_timesteps=10e8#1e10 \n",
    "        self.max_episodes=0 \n",
    "        self.max_iters=0 \n",
    "        self.max_seconds=0  # time constraint\n",
    "        self.callback=None # you can do anything in the callback, since it takes locals(), globals()\n",
    "        self.adam_epsilon=1e-5\n",
    "        self.schedule='linear' # annealing for stepsize parameters (epsilon and adam)\n",
    "\n",
    "\n",
    "def policy_fn(name, ob_space, ac_space):\n",
    "        return MlpPolicy_new(name=name, ob_space=ob_space, ac_space=ac_space,\n",
    "            hid_size=64, num_hid_layers=2)\n",
    "    \n",
    "parg = pargm()\n",
    "agent2 = learning_agent('pi2', env2, policy_fn, parg)\n",
    "agent2.restore('walker_mem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class action_space(object):\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.high = np.array([ 1,  1,  1,  1,  1])\n",
    "        self.low = -np.array([ 1,  1,  1,  1,  1])\n",
    "        self.shape = env.observation_space.shape\n",
    "    \n",
    "    def sample(self):\n",
    "    \n",
    "        return self.env.observation_space.sample()\n",
    "        \n",
    "        \n",
    "class adversial_env(object):\n",
    "    def __init__(self,ag):\n",
    "        # parameter\n",
    "        self.env = gym.make(\"RoboschoolWalker2d-v1\")\n",
    "        self.env.seed(0)\n",
    "        self.ratio = 0.05 # change to 0.1 later in this script\n",
    "        self.threshold = np.array([ 0.058152  ,  0.        ,  0.        ,  1.01691496,  0.        ,\n",
    "        1.0058738 ,  0.        ,  0.12371043, 0,  0,\n",
    "        0.44639227,  0,  0,  0,  0.34056485,\n",
    "        0,  0,  0.23712605,  0,  0,\n",
    "        0,  0])\n",
    "        self.max_turn = 1000\n",
    "        self.combine_ratio = 0.05\n",
    "        \n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.env.observation_space.shape[0],))\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.agent = ag\n",
    "        self.obsr = 0\n",
    "        self.epi_num = 0\n",
    "        self.total_score = 0\n",
    "        self.first = True\n",
    "        self.run_avg = 0\n",
    "        self.rvg_list = []\n",
    "        self.score_list = []\n",
    "        self.epi_list = []\n",
    "        self.env.metadata\n",
    "    \n",
    "    # define reward function\n",
    "    def reward(self, st):\n",
    "        return np.abs(st[3])+0.2*np.abs(st[1])-0.08\n",
    "    \n",
    "    def step(self, a):\n",
    "        self.epi_num = self.epi_num + 1\n",
    "        \n",
    "        obs = np.clip(a,-1,1)*self.threshold*self.ratio + self.obsr\n",
    "        \n",
    "        ac = self.agent.action_ev(obs)\n",
    "        self.obsr, r, done, _ = self.env.step(ac)\n",
    "        #print( np.clip(a,-1,1),np.clip(a,-1,1)*self.ratio)\n",
    "        \n",
    "        if self.epi_num >= self.max_turn:\n",
    "            done = True\n",
    "        \n",
    "        if self.first and done: ###################################\n",
    "            self.first = False\n",
    "            self.run_avg = self.total_score\n",
    "            self.score_list = [self.total_score]\n",
    "            self.epi_list = [self.epi_num]\n",
    "            print(self.run_avg, self.score_list, self.epi_list)\n",
    "            \n",
    "        \n",
    "        final_r = -r\n",
    "        if done and self.epi_num < self.max_turn:\n",
    "            final_r = 15 # terminal cost \n",
    "        \n",
    "        self.total_score += final_r\n",
    "        return self.obsr, final_r, done, 0\n",
    "        \n",
    "        \n",
    "    def seed(self, a):\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.obsr = self.env.reset()\n",
    "        #print(self.total_score)\n",
    "        self.run_avg = (self.combine_ratio*self.total_score) + (1-self.combine_ratio)*self.run_avg\n",
    "        #print(self.run_avg)\n",
    "        #print(self.epi_num)\n",
    "        \n",
    "        if not self.first: #########################################\n",
    "            self.rvg_list.append(self.run_avg)\n",
    "            self.score_list.append(self.total_score)\n",
    "            self.epi_list.append(self.epi_num)\n",
    "        \n",
    "        self.epi_num = 0\n",
    "        self.total_score = 0\n",
    "        return self.obsr\n",
    "    \n",
    "    def result_plot(self):\n",
    "        fon_size = 19\n",
    "        x = list(range(0, len(self.score_list[1:])))\n",
    "        fig=plt.figure(figsize=(18, 4), dpi= 80, facecolor='w', edgecolor='k')\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.scatter(x,self.score_list[1:], s=5)\n",
    "        plt.xlabel('episodes',fontsize=fon_size)\n",
    "        plt.ylabel('total reward',fontsize=fon_size)\n",
    "        #plt.subplot(1,3,2)\n",
    "        #plt.plot(self.rvg_list[1:])\n",
    "        #plt.xlabel('episodes')\n",
    "        #plt.ylabel('running average reward')\n",
    "        plt.subplot(1,2,2)\n",
    "        x = list(range(0, len(self.epi_list)))\n",
    "        plt.scatter(x,self.epi_list, s=5)\n",
    "        plt.xlabel('episodes',fontsize=fon_size)\n",
    "        plt.ylabel('time steps',fontsize=fon_size)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adversarial evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import scipy.optimize\n",
    "import roboschool\n",
    "from agent_file import agent\n",
    "import numpy as np\n",
    "import random\n",
    "from expert import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from gym import spaces\n",
    "plt.style.use('seaborn-white')\n",
    "sns.set(context = \"paper\", font = \"monospace\", font_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class par(object):\n",
    "    def __init__(self):\n",
    "        self.gamma = 0.995\n",
    "        self.env_name = \"Reacher-v1\"\n",
    "        self.tau = 0.97\n",
    "        self.l2_reg = 1e-3\n",
    "        self.max_kl = 1e-2\n",
    "        self.damping = 1e-1\n",
    "        self.seed = 543\n",
    "        self.batch_size = 25000\n",
    "        self.max_epi = 6000\n",
    "        self.log_interval = 1\n",
    "        self.max_avg = 55\n",
    "        \n",
    "args = par()\n",
    "env = adversial_env(agent2)\n",
    "env.ratio = 0.1\n",
    "agn = agent(env, args)\n",
    "agn.load_model('adv_agent_model/adversarial_agent2_distr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thres = np.array([ 0.058152  ,  0.        ,  0.        ,  1.01691496,  0.        ,\n",
    "        1.0058738 ,  0.        ,  0.12371043, 0,  0,\n",
    "        0.44639227,  0,  0,  0,  0.34056485,\n",
    "        0,  0,  0.23712605,  0,  0,\n",
    "        0,  0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = adversial_env(agent2)\n",
    "env.ratio = 0.05\n",
    "action = []\n",
    "for i in range(1000):\n",
    "    score = 0\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    itr = 0\n",
    "    do = False\n",
    "    time = 0\n",
    "    while done == False:   \n",
    "        a = agn.select_action_deterministic(obs)\n",
    "        action.append(np.clip(a,-1,1)*thres*0.05)\n",
    "        time +=1\n",
    "        obs, r, done, _ = env.step(a)\n",
    "        if done:\n",
    "            do = True\n",
    "\n",
    "        score += r\n",
    "    #print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.result_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_num = 50\n",
    "sensor_array = np.array(action)\n",
    "fon_size = 17\n",
    "sns.set(context = \"paper\", font = \"monospace\", font_scale=1.3)\n",
    "fig=plt.figure(figsize=(18, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(1,5,1)\n",
    "plt.hist(sensor_array[:,0], bins = bin_num)\n",
    "plt.xlabel('sensor 1')\n",
    "\n",
    "plt.subplot(1,5,2)\n",
    "plt.hist(sensor_array[:,1], bins = bin_num)\n",
    "plt.xlabel('sensor 2')\n",
    "\n",
    "plt.subplot(1,5,3)\n",
    "plt.hist(sensor_array[:,2], bins = bin_num)\n",
    "plt.xlabel('sensor 3')\n",
    "\n",
    "plt.subplot(1,5,4)\n",
    "plt.hist(sensor_array[:,3], bins = bin_num)\n",
    "plt.xlabel('sensor 4')\n",
    "\n",
    "plt.subplot(1,5,5)\n",
    "plt.hist(sensor_array[:,4], bins = bin_num)\n",
    "plt.xlabel('sensor 5')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "bin_num = 50\n",
    "sensor_array = np.array(action)\n",
    "fon_size = 17\n",
    "sns.set(context = \"paper\", font = \"monospace\", font_scale=1.3)\n",
    "fig=plt.figure(figsize=(18, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(1,5,1)\n",
    "plt.hist(sensor_array[:,5], bins = bin_num)\n",
    "plt.xlabel('sensor 6')\n",
    "\n",
    "plt.subplot(1,5,2)\n",
    "plt.hist(sensor_array[:,6], bins = bin_num)\n",
    "plt.xlabel('sensor 7')\n",
    "\n",
    "plt.subplot(1,5,3)\n",
    "plt.hist(sensor_array[:,7], bins = bin_num)\n",
    "plt.xlabel('sensor 8')\n",
    "\n",
    "plt.subplot(1,5,4)\n",
    "plt.hist(sensor_array[:,8], bins = bin_num)\n",
    "plt.xlabel('sensor 9')\n",
    "\n",
    "plt.subplot(1,5,5)\n",
    "plt.hist(sensor_array[:,9], bins = bin_num)\n",
    "plt.xlabel('sensor 10')\n",
    "\n",
    "plt.show()\n",
    "bin_num = 50\n",
    "sensor_array = np.array(action)\n",
    "fon_size = 17\n",
    "sns.set(context = \"paper\", font = \"monospace\", font_scale=1.3)\n",
    "fig=plt.figure(figsize=(18, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(1,5,1)\n",
    "plt.hist(sensor_array[:,10], bins = bin_num)\n",
    "plt.xlabel('sensor 11')\n",
    "\n",
    "plt.subplot(1,5,2)\n",
    "plt.hist(sensor_array[:,11], bins = bin_num)\n",
    "plt.xlabel('sensor 12')\n",
    "\n",
    "plt.subplot(1,5,3)\n",
    "plt.hist(sensor_array[:,12], bins = bin_num)\n",
    "plt.xlabel('sensor 13')\n",
    "\n",
    "plt.subplot(1,5,4)\n",
    "plt.hist(sensor_array[:,13], bins = bin_num)\n",
    "plt.xlabel('sensor 14')\n",
    "\n",
    "plt.subplot(1,5,5)\n",
    "plt.hist(sensor_array[:,14], bins = bin_num)\n",
    "plt.xlabel('sensor 15')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "bin_num = 50\n",
    "sensor_array = np.array(action)\n",
    "fon_size = 17\n",
    "sns.set(context = \"paper\", font = \"monospace\", font_scale=1.3)\n",
    "fig=plt.figure(figsize=(18, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(1,5,1)\n",
    "plt.hist(sensor_array[:,15], bins = bin_num)\n",
    "plt.xlabel('sensor 16')\n",
    "\n",
    "plt.subplot(1,5,2)\n",
    "plt.hist(sensor_array[:,16], bins = bin_num)\n",
    "plt.xlabel('sensor 17')\n",
    "\n",
    "plt.subplot(1,5,3)\n",
    "plt.hist(sensor_array[:,17], bins = bin_num)\n",
    "plt.xlabel('sensor 18')\n",
    "\n",
    "plt.subplot(1,5,4)\n",
    "plt.hist(sensor_array[:,18], bins = bin_num)\n",
    "plt.xlabel('sensor 19')\n",
    "\n",
    "plt.subplot(1,5,5)\n",
    "plt.hist(sensor_array[:,19], bins = bin_num)\n",
    "plt.xlabel('sensor 20')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "bin_num = 50\n",
    "sensor_array = np.array(action)\n",
    "fon_size = 17\n",
    "sns.set(context = \"paper\", font = \"monospace\", font_scale=1.3)\n",
    "fig=plt.figure(figsize=(18, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(1,5,1)\n",
    "plt.hist(sensor_array[:,20], bins = bin_num)\n",
    "plt.xlabel('sensor 21')\n",
    "\n",
    "plt.subplot(1,5,2)\n",
    "plt.hist(sensor_array[:,21], bins = bin_num)\n",
    "plt.xlabel('sensor 22')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_num = 50\n",
    "sensor_array = np.array(action)\n",
    "fon_size = 17\n",
    "sns.set(context = \"paper\", font = \"monospace\", font_scale=1.3)\n",
    "fig=plt.figure(figsize=(18, 5), dpi= 80, facecolor='w', edgecolor='k')\n",
    "plt.subplot(1,4,1)\n",
    "plt.hist(sensor_array[:,0], bins = bin_num)\n",
    "plt.xlabel('sensor 1')\n",
    "\n",
    "plt.subplot(1,4,2)\n",
    "plt.hist(sensor_array[:,3], bins = bin_num)\n",
    "plt.xlabel('sensor 4')\n",
    "\n",
    "plt.subplot(1,4,3)\n",
    "plt.hist(sensor_array[:,7], bins = bin_num)\n",
    "plt.xlabel('sensor 8')\n",
    "\n",
    "plt.subplot(1,4,4)\n",
    "plt.hist(sensor_array[:,14], bins = bin_num)\n",
    "plt.xlabel('sensor 15')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
